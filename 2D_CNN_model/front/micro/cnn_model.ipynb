{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import contextlib\n",
    "from datetime import datetime\n",
    "from IPython import get_ipython\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../utils')  # Add the folder to the path\n",
    "from notebook_saver import NotebookSaver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_start_index = len(get_ipython().user_ns['In'])\n",
    "\n",
    "# References: https://pythonprogramming.net/convolutional-neural-network-deep-learning-python-tensorflow-keras/\n",
    "DATADIR = r\"C:\\Users\\New Asus\\Documents\\FIT4701_2025_Sem1\\training_img_dataset\\front_dataset\\front_micro\\dwt_training_picture\"\n",
    "CATEGORIES = [\"clap\", \"punching\", \"pushpull\", \"rubhand\", \"waving\"]\n",
    "\n",
    "notebook_saver = NotebookSaver()\n",
    "save_dir = notebook_saver.get_save_dir()\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DATADIR,category)\n",
    "    for img in os.listdir(path):\n",
    "        img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)\n",
    "        break\n",
    "    break \n",
    "\n",
    "IMG_SIZE = 64\n",
    "\n",
    "new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "training_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES:\n",
    "\n",
    "        path = os.path.join(DATADIR,category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "\n",
    "        for img in tqdm(os.listdir(path)):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE)) \n",
    "                training_data.append([new_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "create_training_data()\n",
    "print(len(training_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for features,label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "    \n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_out = open(\"X.pickle\",\"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y.pickle\",\"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_in = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255.0\n",
    "y = to_categorical(y, num_classes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping for preventing overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential([\n",
    "    \n",
    "    # References: Using dropouts in conv layers: https://link.springer.com/chapter/10.1007/978-3-319-54184-6_12#Sec4 \n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'), # Hybrid dropouts\n",
    "    tf.keras.layers.Dropout(0.5), # References: Improving neural networks by preventing co-adaptation of feature detectors: https://arxiv.org/pdf/1207.0580 \n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "cnn.compile(Adam(learning_rate = 0.0001),  \n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "history = cnn.fit(X_train, y_train, epochs=200, batch_size=64, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "val_loss, val_acc = cnn.evaluate(X_val, y_val) \n",
    "print(f\"Final Validation Accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss and accuracy graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.suptitle('Optimizer : Adam', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "notebook_saver.save_plot(name = 'plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix:\n",
    "# Get predictions from the model on the validation set\n",
    "y_pred_probs = cnn.predict(X_val)\n",
    "\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # Convert probabilities to class indices\n",
    "\n",
    "# Convert one-hot encoded true labels to class indices\n",
    "y_true = np.argmax(y_val, axis=1)\n",
    "\n",
    "# Reference: https://github.com/parisafm/CSI-HAR-Dataset/blob/main/CNN.py\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i,j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i,j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    notebook_saver.save_plot(name = 'cm')\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=CATEGORIES, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model classfication report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_true, y_pred, target_names=CATEGORIES)\n",
    "print(\"Classification Report:\\n\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unused part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Create the Feature Extractor Model from the trained CNN\n",
    "# #    We take layers up to Flatten (index -3)\n",
    "# if len(cnn.layers) < 3:\n",
    "#     print(\"Error: CNN model is too shallow for feature extraction at Flatten layer.\")\n",
    "# else:\n",
    "#     feature_extractor = Model(inputs=cnn.input,\n",
    "#                               outputs=cnn.layers[-3].output, # Output of the Flatten layer\n",
    "#                               name=\"CNN_Feature_Extractor\")\n",
    "#     feature_extractor.summary() # Show the structure of the extractor\n",
    "\n",
    "#     # 2. Extract Features\n",
    "#     print(\"\\n--- Extracting features using the trained CNN ---\")\n",
    "#     print(\"Extracting features from training data...\")\n",
    "#     X_train_features = feature_extractor.predict(X_train)\n",
    "#     print(\"Extracting features from validation data...\")\n",
    "#     X_val_features = feature_extractor.predict(X_val)\n",
    "\n",
    "#     print(f\"Shape of extracted features (train): {X_train_features.shape}\") # e.g., (num_samples, num_flattened_features)\n",
    "#     print(f\"Shape of extracted features (validation): {X_val_features.shape}\")\n",
    "\n",
    "#     # 3. Apply PCA\n",
    "#     n_components = 64 # Hyperparameter: Choose number of components (e.g., 50, 64, 128, or based on explained variance)\n",
    "#     if X_train_features.shape[1] < n_components:\n",
    "#         print(f\"Warning: n_components ({n_components}) is >= number of features ({X_train_features.shape[1]}). Setting n_components to {X_train_features.shape[1] - 1}.\")\n",
    "#         n_components = X_train_features.shape[1] - 1 if X_train_features.shape[1] > 1 else 1\n",
    "\n",
    "\n",
    "#     if n_components > 0: # Proceed only if PCA makes sense\n",
    "#         pca = PCA(n_components=n_components, random_state=42)\n",
    "\n",
    "#         print(f\"\\n--- Applying PCA to reduce dimensionality to {n_components} components ---\")\n",
    "#         print(\"Fitting PCA on extracted training features...\")\n",
    "#         pca.fit(X_train_features) # Fit ONLY on training data\n",
    "\n",
    "#         print(\"Transforming features using PCA...\")\n",
    "#         X_train_pca = pca.transform(X_train_features)\n",
    "#         X_val_pca = pca.transform(X_val_features)\n",
    "\n",
    "#         print(f\"Shape after PCA (train): {X_train_pca.shape}\")\n",
    "#         print(f\"Shape after PCA (validation): {X_val_pca.shape}\")\n",
    "#         print(f\"Explained variance ratio by {n_components} components: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "\n",
    "#         # 4. Train a Simpler Classifier (Simple MLP) on PCA features\n",
    "#         print(\"\\n--- Defining and Training a Simple MLP on PCA features ---\")\n",
    "#         mlp_on_pca = tf.keras.models.Sequential([\n",
    "#             tf.keras.layers.Input(shape=(n_components,)), # Input shape is now n_components\n",
    "#             tf.keras.layers.Dense(32, activation='relu'),   # Smaller Dense layer\n",
    "#             tf.keras.layers.Dropout(0.3),                   # Optional Dropout\n",
    "#             tf.keras.layers.Dense(len(CATEGORIES), activation='softmax') # Output layer\n",
    "#         ], name=\"MLP_on_PCA_Features\")\n",
    "\n",
    "#         mlp_on_pca.compile(Adam(learning_rate = 0.001), # Can use slightly higher LR maybe\n",
    "#                            loss='categorical_crossentropy',\n",
    "#                            metrics=['accuracy'])\n",
    "\n",
    "#         mlp_on_pca.summary()\n",
    "\n",
    "#         # Use early stopping for the MLP as well\n",
    "#         mlp_early_stopping = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True) # Shorter patience maybe\n",
    "\n",
    "#         history_mlp = mlp_on_pca.fit(X_train_pca, y_train, # Use PCA features and original one-hot labels\n",
    "#                                      epochs=40, # Train for fewer epochs maybe\n",
    "#                                      batch_size=64,\n",
    "#                                      validation_data=(X_val_pca, y_val),\n",
    "#                                      callbacks=[mlp_early_stopping])\n",
    "\n",
    "\n",
    "#         # 5. Evaluate the MLP on PCA features\n",
    "#         print(\"\\n--- Evaluating the Simple MLP on PCA features ---\")\n",
    "#         val_loss_mlp, val_acc_mlp = mlp_on_pca.evaluate(X_val_pca, y_val)\n",
    "#         print(f\"MLP on PCA Features - Final Validation Loss: {val_loss_mlp}\")\n",
    "#         print(f\"MLP on PCA Features - Final Validation Accuracy: {val_acc_mlp}\")\n",
    "\n",
    "\n",
    "#         # --- Optional: Plotting for MLP on PCA ---\n",
    "#         print(\"\\n--- Plotting MLP on PCA History ---\")\n",
    "#         plt.figure(figsize=(12, 5))\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         plt.ylabel('Loss', fontsize=14)\n",
    "#         plt.plot(history_mlp.history['loss'], label='Training Loss')\n",
    "#         plt.plot(history_mlp.history['val_loss'], label='Validation Loss')\n",
    "#         plt.legend(loc='upper right')\n",
    "#         plt.title('MLP on PCA Features Loss')\n",
    "\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         plt.ylabel('Accuracy', fontsize=14)\n",
    "#         plt.plot(history_mlp.history['accuracy'], label='Training Accuracy')\n",
    "#         plt.plot(history_mlp.history['val_accuracy'], label='Validation Accuracy')\n",
    "#         plt.legend(loc='lower right')\n",
    "#         plt.title('MLP on PCA Features Accuracy')\n",
    "#         plt.tight_layout()\n",
    "#         notebook_saver.save_plot(name='mlp_on_pca_plot')\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "#         # --- Optional: Confusion Matrix for MLP on PCA ---\n",
    "#         print(\"\\n--- Generating Confusion Matrix for MLP on PCA ---\")\n",
    "#         y_pred_probs_mlp = mlp_on_pca.predict(X_val_pca)\n",
    "#         y_pred_mlp = np.argmax(y_pred_probs_mlp, axis=1)\n",
    "#         # y_true is the same as before (from original y_val)\n",
    "\n",
    "#         # Compute confusion matrix\n",
    "#         cm_mlp = confusion_matrix(y_true, y_pred_mlp)\n",
    "\n",
    "#         # Plot normalized confusion matrix\n",
    "#         plot_confusion_matrix(cm_mlp, classes=CATEGORIES, title='MLP on PCA Features - Normalized Confusion Matrix')\n",
    "#         plt.show()\n",
    "\n",
    "#         # Classification Report for MLP on PCA\n",
    "#         print(\"\\n--- Classification Report for MLP on PCA Features ---\")\n",
    "#         report_mlp = classification_report(y_true, y_pred_mlp, target_names=CATEGORIES, zero_division=0)\n",
    "#         print(report_mlp)\n",
    "\n",
    "#     else: # Case where n_components <= 0\n",
    "#          print(\"\\nSkipping PCA and MLP training because n_components is not positive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Comparison\n",
    "# print(\"\\n\\n--- Final Accuracy Comparison ---\")\n",
    "# print(f\"Original CNN Final Validation Accuracy: {val_acc:.4f}\")\n",
    "# if 'val_acc_mlp' in locals(): # Check if MLP was trained\n",
    "#     print(f\"MLP on PCA Features Validation Accuracy: {val_acc_mlp:.4f}\")\n",
    "# else:\n",
    "#     print(\"MLP on PCA Features was not run.\")\n",
    "\n",
    "# print(\"\\n--- Script Finished ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save code, model summary and training output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_saver.save_notebook_code(run_start_index)\n",
    "notebook_saver.save_model_summary(cnn)\n",
    "notebook_saver.save_training_output(history, val_loss, val_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
