{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0123b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to root folder\n",
    "# %cd \"C:\\Users\\New Asus\\Documents\\FIT4701_2025_Sem1\\SHD-HAR-Dataset\\SHD-HAR-Dataset-main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e4ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcbdd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from math import sqrt, atan2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ced5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b511f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8ce89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d93979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0474f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESP32:\n",
    "    \"\"\"Parse ESP32 Wi-Fi Channel State Information (CSI) obtained using ESP32 CSI Toolkit by Hernandez and Bulut.\n",
    "    ESP32 CSI Toolkit: https://stevenmhernandez.github.io/ESP32-CSI-Tool/\n",
    "    \"\"\"\n",
    "\n",
    "    # View README.md for more information on null subcarriers\n",
    "    NULL_SUBCARRIERS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 64, 65, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 382, 383]\n",
    "\n",
    "    def __init__(self, csi_file):\n",
    "        self.csi_file = csi_file\n",
    "        self.__read_file()\n",
    "    \n",
    "    def __read_file(self):\n",
    "        \"\"\"Read RAW CSI file (.csv) using Pandas and return a Pandas dataframe\n",
    "        \"\"\"\n",
    "        self.csi_df = pd.read_csv(self.csi_file)\n",
    "\n",
    "    def seek_file(self):\n",
    "        \"\"\"Seek RAW CSI file\n",
    "        \"\"\"\n",
    "        return self.csi_df\n",
    "\n",
    "    def filter_by_sig_mode(self, sig_mode):\n",
    "        \"\"\"Filter CSI data by signal mode\n",
    "        Args:  \n",
    "            sig_mode (int):\n",
    "            0 : Non - High Throughput Signals (non-HT)\n",
    "            1 : HIgh Throughput Signals (HT)\n",
    "        \"\"\"\n",
    "        self.csi_df = self.csi_df.loc[self.csi_df['sig_mode'] == sig_mode]\n",
    "        return self\n",
    "\n",
    "    def get_csi(self):\n",
    "        \"\"\"Read CSI string as Numpy array\n",
    "\n",
    "        The CSI data collected by ESP32 contains channel frequency responses (CFR) represented by two signed bytes (imaginary, real) for each sub-carriers index\n",
    "        The length (bytes) of the CSI sequency depends on the CFR type\n",
    "        CFR consist of legacy long training field (LLTF), high-throughput LTF (HT-LTF), and space- time block code HT-LTF (STBC-HT-LTF)\n",
    "        Ref: https://docs.espressif.com/projects/esp-idf/en/latest/esp32/api-guides/wifi.html#wi-fi-channel-state-information\n",
    "\n",
    "        NOTE: Not all 3 field may not be present (as represented in table and configuration)\n",
    "        \"\"\"\n",
    "        raw_csi_data = self.csi_df['data'].copy()\n",
    "        csi_data = np.array([np.fromstring(csi_datum.strip('[ ]'), dtype=int, sep = ',') for csi_datum in raw_csi_data])\n",
    "        self.csi_data = csi_data\n",
    "        return self\n",
    "\n",
    "    # NOTE: Currently does not provide support for all signal subcarrier types\n",
    "    def remove_null_subcarriers(self):\n",
    "        \"\"\"Remove NULL subcarriers from CSI\n",
    "        \"\"\"\n",
    "\n",
    "        # Non-HT Signals (20 Mhz) - non STBC\n",
    "        if self.csi_data.shape[1] == 128:\n",
    "            remove_null_subcarriers = self.NULL_SUBCARRIERS[:24]\n",
    "        # HT Signals (40 Mhz) - non STBC\n",
    "        elif self.csi_data.shape[1] == 384:\n",
    "            remove_null_subcarriers = self.NULL_SUBCARRIERS\n",
    "        else:\n",
    "            return self\n",
    "\n",
    "        csi_data_T = self.csi_data.T\n",
    "        csi_data_T_clean = np.delete(csi_data_T, remove_null_subcarriers, 0)\n",
    "        csi_data_clean = csi_data_T_clean.T\n",
    "        self.csi_data = csi_data_clean\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_amplitude_from_csi(self):\n",
    "        \"\"\"Calculate the Amplitude (or Magnitude) from CSI\n",
    "        Ref: https://farside.ph.utexas.edu/teaching/315/Waveshtml/node88.html\n",
    "        \"\"\"\n",
    "        amplitude = np.array([np.sqrt(data[::2]**2 + data[1::2]**2) for data in self.csi_data])\n",
    "        self.amplitude = amplitude\n",
    "        return self\n",
    "\n",
    "    def get_phase_from_csi(self):\n",
    "        \"\"\"Calculate the Amplitude (or Magnitude) from CSI\n",
    "        Ref: https://farside.ph.utexas.edu/teaching/315/Waveshtml/node88.html\n",
    "        \"\"\"\n",
    "        phase = np.array([np.arctan2(data[::2], data[1::2]) for data in self.csi_data])\n",
    "        self.phase = phase\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64fdf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amplitude_plot(amp, start_stamp = 0, num_packets = 1000, plot_name = \"\"):\n",
    "    \"\"\"\n",
    "    plotting function for visualizing subcarrier amplitude per packet\n",
    "    amp: csi amplitude array\n",
    "    start_stamp: start plot from which packet\n",
    "    num_packet: plot how many packets\n",
    "    plot_name: name of activity being plotted   \n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    # number of subcarriers\n",
    "    num_lines = amp.shape[1]\n",
    "\n",
    "    # setup color map\n",
    "    cmap = plt.cm.hsv\n",
    "    norm = np.linspace(0, 1, num_lines)\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(0, 1))\n",
    "    \n",
    "    # setup plot\n",
    "    plt.figure(figsize=(15,10))\n",
    "    df = np.asarray(amp, dtype=np.int32)\n",
    "\n",
    "    # plot each subcarrier\n",
    "    for i in range(num_lines):   \n",
    "        plt.plot(range(num_packets), df[start_stamp:start_stamp+num_packets, i], color= sm.to_rgba(norm[i]), linewidth = 0.5)\n",
    "        \n",
    "    # attach labels and limitations\n",
    "    plt.xlabel(\"Packet\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.xlim(0, num_packets)\n",
    "    plt.title(f\"Amplitude-Packet plot ({plot_name})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05392f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(X, variance_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Apply PCA and determine the number of components (k) to keep based on a variance threshold.\n",
    "    \n",
    "    Args:\n",
    "        X: Tensor of shape (n_samples, n_features), your input data.\n",
    "        variance_threshold: Float, target cumulative variance (e.g., 0.95 for 95%).\n",
    "    \n",
    "    Returns:\n",
    "        reduced_X: Tensor with reduced dimensions.\n",
    "        k: Integer, number of components retained.\n",
    "    \"\"\"\n",
    "    # Center the data\n",
    "    mean = tf.reduce_mean(X, axis=0)\n",
    "    centered_X = X - mean\n",
    "    \n",
    "    # Perform SVD\n",
    "    s, u, v = tf.linalg.svd(centered_X, full_matrices=False)\n",
    "    \n",
    "    # Compute explained variance ratio\n",
    "    explained_variance_ratio = s**2 / tf.reduce_sum(s**2)\n",
    "    \n",
    "    # Compute cumulative explained variance\n",
    "    cumulative_variance = tf.cumsum(explained_variance_ratio)\n",
    "    \n",
    "    # Find k where cumulative variance meets or exceeds the threshold\n",
    "    k = tf.argmax(cumulative_variance >= variance_threshold) + 1\n",
    "    k = tf.cast(k, tf.int32)  # Ensure k is an integer\n",
    "    \n",
    "    # Reduce the data to k components\n",
    "    reduced_X = tf.matmul(centered_X, v[:, :k])\n",
    "    \n",
    "    return reduced_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc97998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_amplitude(directory):\n",
    "    \"\"\"\n",
    "    function to convert directory of raw csv files to ampltude csv files\n",
    "    directory: directory containing raw csv files\n",
    "    \"\"\"\n",
    "    # Walk through the directory and its subdirectories\n",
    "    for dirpath, _, filenames in os.walk(directory):\n",
    "        # Filter out CSV files\n",
    "        csv_files = [filename for filename in filenames if filename.endswith('.csv')]\n",
    "        \n",
    "        if csv_files:  # Proceed only if CSV files are found in the current directory\n",
    "            # Create the output directory if it doesn't exist\n",
    "            output_directory = os.path.join(dirpath, 'amplitude')\n",
    "            os.makedirs(output_directory, exist_ok=True)\n",
    "            \n",
    "            # Process each CSV file\n",
    "            for csv_file in csv_files:\n",
    "                # Construct the full path to the CSV file\n",
    "                csv_file_path = os.path.join(dirpath, csv_file)\n",
    "                \n",
    "                # Construct the output CSV file path\n",
    "                output_csv_file = os.path.join(output_directory, os.path.splitext(csv_file)[0] + \".csv\")\n",
    "                \n",
    "                # Load the matrix from the CSV file\n",
    "                matrix = (ESP32(csv_file_path)\n",
    "                          .filter_by_sig_mode(1)\n",
    "                          .get_csi()\n",
    "                          .remove_null_subcarriers()\n",
    "                          .get_amplitude_from_csi().amplitude)\n",
    "                \n",
    "                # Adjust the decimals according to your needs\n",
    "                matrix = np.round(matrix, decimals = 5)\n",
    "                \n",
    "                # Write each row of the matrix to the output CSV file sequentially\n",
    "                with open(output_csv_file, 'w', newline='') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    for row in matrix:\n",
    "                        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4d5d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def directory_to_heatmap(directory, dim = 64):\n",
    "    \"\"\"\n",
    "    function to convert amplitude files to heatmap jpg\n",
    "    directory: amplitude file directory\n",
    "    dim: length and width of jpg\n",
    "    \"\"\"\n",
    "    # Walk through the directory and its subdirectories\n",
    "    for dirpath, _, filenames in os.walk(directory):\n",
    "        # Filter out CSV files\n",
    "        csv_files = [filename for filename in filenames if filename.endswith('.csv')]\n",
    "        \n",
    "        if csv_files:  # Proceed only if CSV files are found in the current directory\n",
    "            # Create the output directory if it doesn't exist\n",
    "            path_components = os.path.split(dirpath)\n",
    "            output_head = path_components[0] + \"_picture\"\n",
    "            # Indicate progress\n",
    "            print(f\"{path_components[1]} start\")\n",
    "            output_directory = os.path.join(output_head, path_components[1])\n",
    "            os.makedirs(output_directory, exist_ok=True)\n",
    "            \n",
    "            # Process each CSV file\n",
    "            for csv_file in csv_files:\n",
    "                # Construct the full path to the CSV file\n",
    "                csv_file_path = os.path.join(dirpath, csv_file)\n",
    "                \n",
    "                # Construct the output image path\n",
    "                output_image = os.path.join(output_directory, os.path.splitext(csv_file)[0] + \".jpg\")\n",
    "                \n",
    "                # Load the matrix from the CSV file\n",
    "                matrix = pd.read_csv(csv_file_path, header=None) \n",
    "                matrix = matrix.astype(float)\n",
    "    \n",
    "                # Normalize data\n",
    "                scaler = MinMaxScaler()\n",
    "                scaled_matrix = scaler.fit_transform(matrix)\n",
    "    \n",
    "                # Resize data using OpenCV\n",
    "                resized_matrix = cv2.resize(scaled_matrix, (dim, dim))\n",
    "\n",
    "                # Reshape data for image\n",
    "                image_data = resized_matrix.reshape(1, *resized_matrix.shape, 1)\n",
    "    \n",
    "                # Create ImageDataGenerator\n",
    "                datagen = ImageDataGenerator(featurewise_center=False)\n",
    "    \n",
    "                # Fit generator to data\n",
    "                datagen.fit(image_data)\n",
    "    \n",
    "                # Generate heatmap image\n",
    "                for X_batch in datagen.flow(image_data, batch_size=1):\n",
    "                    heatmap_image = X_batch.squeeze()\n",
    "                    break  # only generate one batch\n",
    "    \n",
    "                # Plot and save heatmap\n",
    "                plt.imshow(heatmap_image, cmap='hot', interpolation='nearest')\n",
    "                plt.axis('off')\n",
    "                plt.savefig(output_image, bbox_inches='tight')  # Save as .jpg file\n",
    "                plt.close()\n",
    "            # Indicate progress\n",
    "            print(f\"{path_components[1]} done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f3ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_amplitude(directory):\n",
    "    \"\"\"\n",
    "    function to convert directory of raw csv files to ampltude csv files\n",
    "    directory: directory containing raw csv files\n",
    "    \"\"\"\n",
    "    # Walk through the directory and its subdirectories\n",
    "    for dirpath, _, filenames in os.walk(directory):\n",
    "        # Filter out CSV files\n",
    "        csv_files = [filename for filename in filenames if filename.endswith('.csv')]\n",
    "        \n",
    "        if csv_files:  # Proceed only if CSV files are found in the current directory\n",
    "            # Create the output directory if it doesn't exist\n",
    "            output_directory = os.path.join(dirpath, 'amplitude')\n",
    "            os.makedirs(output_directory, exist_ok=True)\n",
    "            \n",
    "            # Process each CSV file\n",
    "            for csv_file in csv_files:\n",
    "                # Construct the full path to the CSV file\n",
    "                csv_file_path = os.path.join(dirpath, csv_file)\n",
    "                \n",
    "                # Construct the output CSV file path\n",
    "                output_csv_file = os.path.join(output_directory, os.path.splitext(csv_file)[0] + \".csv\")\n",
    "                \n",
    "                matrix = np.loadtxt(csv_file_path, delimiter=',')\n",
    "                matrix = tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
    "                pca = PCA(n_components=0.95)\n",
    "                matrix = pca.fit_transform(matrix)\n",
    "                # matrix = apply_pca(matrix, variance_threshold=0.95)\n",
    "                \n",
    "                # Write each row of the matrix to the output CSV file sequentially\n",
    "                with open(output_csv_file, 'w', newline='') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    for row in matrix:\n",
    "                        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_amplitude(r\"C:\\Users\\New Asus\\Documents\\FIT4701_2025_Sem1\\resources\\SHD-HAR-Dataset\\SHD-HAR-Dataset-main\\amplitude\\front\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6bc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file_path = r\"C:\\Users\\New Asus\\Documents\\FIT4701_2025_Sem1\\resources\\SHD-HAR-Dataset\\SHD-HAR-Dataset-main\\amplitude\\clap_subset\\amplitude\\clap1.csv\"\n",
    "\n",
    "# # Load the matrix from the CSV file\n",
    "# matrix = pd.read_csv(csv_file_path, header=None) \n",
    "# matrix = matrix.astype(float)\n",
    "# # Get the amplitude array\n",
    "# num_packets = min(1000, matrix.shape[0])\n",
    "# amplitude_plot(matrix, start_stamp=0, num_packets=num_packets, plot_name=\"Example Plot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4685e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file_path = r\"C:\\Users\\New Asus\\Documents\\FIT4701_2025_Sem1\\resources\\SHD-HAR-Dataset\\SHD-HAR-Dataset-main\\amplitude\\pca_front\\amplitude\\clap1.csv\"\n",
    "\n",
    "# # Load the matrix from the CSV file\n",
    "# matrix = pd.read_csv(csv_file_path, header=None) \n",
    "# matrix = matrix.astype(float)\n",
    "# # Get the amplitude array\n",
    "# # amp = esp32.amplitude\n",
    "# # print(amp)\n",
    "# num_packets = min(1000, matrix.shape[0])\n",
    "# amplitude_plot(matrix, start_stamp=0, num_packets=num_packets, plot_name=\"Example Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075978ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# esp32 = ESP32(r\"C:\\Users\\New Asus\\Documents\\FIT4701_2025_Sem1\\resources\\SHD-HAR-Dataset\\SHD-HAR-Dataset-main\\raw\\front\\jump\\jump1.csv\")\n",
    "\n",
    "# # Read and filter the dataset (optional)\n",
    "# esp32.get_csi()  # Extract CSI data\n",
    "# esp32.remove_null_subcarriers()  # Remove null subcarriers\n",
    "# esp32.get_amplitude_from_csi()  # Compute amplitude\n",
    "\n",
    "# # Get the amplitude array\n",
    "# amp = esp32.amplitude\n",
    "# print(amp)\n",
    "# num_packets = min(1000, amp.shape[0])\n",
    "# amplitude_plot(amp, start_stamp=0, num_packets=num_packets, plot_name=\"Example Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478b0df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# esp32 = ESP32(r\"C:\\Users\\New Asus\\Documents\\FIT4701_2025_Sem1\\resources\\SHD-HAR-Dataset\\SHD-HAR-Dataset-main\\raw\\front\\clap\\clap1.csv\")\n",
    "\n",
    "# # Read and filter the dataset (optional)\n",
    "# esp32.get_csi()  # Extract CSI data\n",
    "# esp32.remove_null_subcarriers()  # Remove null subcarriers\n",
    "# esp32.get_amplitude_from_csi()  # Compute amplitude\n",
    "\n",
    "# # Get the amplitude array\n",
    "# amp = esp32.amplitude\n",
    "# print(amp)\n",
    "\n",
    "## References: https://github.com/StatQuest/pca_demo/blob/master/pca_demo.py\n",
    "# pca = PCA() # create a PCA object\n",
    "# pca.fit(amp) # do the math\n",
    "# pca_data = pca.transform(amp) # get PCA coordinates for scaled_data\n",
    "\n",
    "# per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)\n",
    "# labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    "\n",
    "# plt.figure(figsize=(100, 6))\n",
    "# plt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)\n",
    "# plt.ylabel('Percentage of Explained Variance')\n",
    "# plt.xlabel('Principal Component')\n",
    "# plt.title('Scree Plot')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38e057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_to_heatmap(r\"C:\\Users\\New Asus\\Documents\\FIT4701_2025_Sem1\\training_img_dataset\\front_dataset\\front_micro\\pca_training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
